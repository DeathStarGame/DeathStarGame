Continuation of:

- [../cloud-native-system/design.md](../cloud-native-system/design.md)
- [../search-for-the-game.md](../search-for-the-game.md)
- [../origin-cluster/origin-cluster.md](../origin-cluster/origin-cluster.md)
- [../as-vscode-extension.md](../deathstar.ltee/as-vscode-extension.md)
- [../deathstar.ltee/design-notes.md](../deathstar.ltee/design-notes.md)
- [../play-scenarios-in-browser/design-notes.md](../play-scenarios-in-browser/design-notes.md)

## deathstar design

<img height="512px" src="../play-scenarios-in-browser/svg/2020-10-26-IPFS-peers.svg"></img>

- app is some form of desktop instance with an IPFS node included
- IPFS node provides networking and idenetity
- through IPFS node we discover other peers, connect to host peers and bidirectionally exchange data
- the UI runs either in browser or in a webrenderer of a desktop app - so web ui in every case
- docker-to-desktop-app exists?  "take this docker compose deploymen and turn into installable updatable app" ?
  - if exists, app can have browser GUI over rsocket (it's just a renderer)
- app has a jvm-app that is always non-binary so that eval works - this app hosts games
- updates : app or deployment should be installable and auto-updatable
- app comes with: IPFS node, jvm-app, ui
- delivery with docker app: build, share, and run a set of microservices as a single entity
  - users will need to install docker, and docker app plugin
  - then, with a single command install an app (or another instance of it) `docker app install myuser/hello-world:0.1.0 --set hello.port=8181`
  - app ui will notify user about updates and show two commands: one to run a new version of an app, and another (after) to uninstall the previous

## stage 1

- add 
  - traefik (serves ui, proxies rsocket to app directly, serves IPFS node ui)
  - ui-prod ui-dev (dev only builds, ui-prod always serves)
  - jvm-app (has rsocket and can talk to ui)
  - IPFS node (just runs on it's own at this point and we can access it's ui via traefik)
- make this one instance launchable: we launch everything, then jvm-app manually
- make several named instances launchable, such that we only toggle jvm-app 

## stage 2

- explore IPFS node

## one way to develop system and scenarios

- developer or not - there should be a unified single way to develop sceanrios
- and naturally, we developers want to develop them within the system (so we start the whole thing and we can delop it and/or scenarios)
- so what we do it build Death Star game, the same process should be exposed to any user if they want to develop their scenarios (choose only how many player instances to launch in docker)
- but, it should be so that they can use their own repositories - or, simply be able once done to take files and put them themselves wherever
- and the system can download sceanrios from any git repo
- so one system, one way to build it and scenarios within it, so anybody is a user/palyer/developer

## with one system, should browser vscode be in its own tab and game gui in its own?

- if so, VSCode will have a minimal extension for it that will carry out ops originating from jvm-app
- jvm-app and vscode may even share a filesystem inside a docker volume
- jvm-app will say to extension "open this and that file for the user" or say "show timer" or may ask "give me that file" (if they don't share fs, otherwise jvm-app will read itlsef, which is preferable)
- users can download files they edit with browser vscode within the system
- and game GUI will be a standalone app communicating with jvm-app to show scenario's graphics and multiplayer etc.

<img height="512px" src="./svg/2020-10-31-IPFS+vscode.svg"></img>

- IPFS nodes are *both* connected to global peer network and local docker network so that the system can be run even offline
  - IPFS node adds a locally resolvable docker network address to the list of default (auto generated by node addresses)
  - even if laptop is offline or elsewise, nodes still discover each other and multiplayer works


## OS windows for seeing the editor and game gui simulteneously?

- open game gui tab in the brower tab in a new browser window taking half-screen (rigth side)
- open editor tab in another browser window taking the left side of the scree
- drag middle boudary to make one or another bigger
- why
  - if we build our game ui and editor as part of it, we dicard all existing tooling and we'll need to make an editor,clj extension, repl
  - can we leverage the fact that editors exist and build on that? e.g. by using vscode-in-the-browser
  - from user experience standpoint: it is almost the same as having our own tabs and windows inside our ui, whereas OS level windows do exactly that
  - another: links and decoupling from single-ui/single tool
  - although ui runs on localhost, links like `game/player/stats/?whatever=3` should be exchangable ideally, like it already works on the web
  - in the game, such link can be part of a another app's ui and will eventually result in a global decentralzied query (or local for starters) and any peer will see the resulting page, so links would work
- if we use browser tabs themselves as tabs, we can rely on link and think in terms of apps
- it's a different way to look at it: we don't build a single ui, but rather a docker system with possible several uis (we already have IPFS ui, game ui)
- in simpler words
  - players will edit code -> that will go to jvm-app -> and it will push state to game gui
  - so although we use two browser windows by means of splitting PC's screen, we input into the same app entity

## source code of the system (DeathStarGame repo) shared between containers? 

- VScode container and jvm-app preferably should share a filesystem, so that only jvm-app did all fs writes
- another thing - when the system runs, ideally, it should be possible to REPL into it using its own VScode browser ui, and for that the source code should come with the system
- another case - when traefik, IPFS , ui-prod ... etc. containers will be lauchned, they will need to access the config files (which are usually COPYd into on image build or passed as docker-compose env variables)
- but what if it was possible to give all caontainers access to a volume of sorts, that would contain the source - github.com/DeathStarGame/DeathStarGame - a central singular instance of all source
- than, every container could access it's config from this root directory (via volume) , and there would be no need to COPY it in every Dockerfile
- can the dev/prod separation be avoided and can the system come as an OS of sorts?
- this way, the ui build container (with shadow-cljs) could output files as is into respective out dirs within DeathStarGame repo tree, and ui-prod could serve that path directly, no need for copying
- another approach: single container with scripts to start/stop binaries (can even consider a scripting alternative to bash as with a container it's a trivial apt install)

## running the system inside a single container

- first of all, it works (no docker-compose files, restart:always and container tools, but scripts instead)
- it works with giving repo as a volume to several containers( ok that build will override files ,.user dirs will be shared),apps will point to the same src code, so restarting apps ok
- needed an alternative to bash - a lisp, preferably clojure - so that all scripts (even in f files when docking containers) were in a sane language
- with one container, we lose cloud tools and have to script, but gain a bit more flexibility(programmability) and a sort of simplicity/singularity
- bash alternative: only if it has *interoperability* with bash, such that we translate bash examples in docs back and forth without guessing

## distinct builder and runner docker-compose services(containers)

- one builder will run shadow-cljs that will compile both ui and vscode extension
- another will build uberjar which will be run from a slimmer jre-only container in release version (if ran without REPLs into system itself)
- all services will share a volume (DeathStarGame repo), builders will output to usual target/out dirs and runners will run from them

## what installing a scenario looks like?

- sidenote: scenario process, running on the server, should control it's own render (even if it's embedded into game gui), so system only talks to one scneario process
- can installing a scenario mean spinning up a container?
- can scenario gui run in a separate tab? 
- can scenario server-side be a nodejs app? a jvm-app?
- so can scanerios be built as apps interacting with DeathStarGame apis
- if scenario is installed as a regular dep, deos it mean (require-ing) it in jvm builder and adding a :build target and compiling renderer in shadow-cljs builder?

## ~~yes, scenarios should be apps, spinned up in containers~~

- <s>we are already in docker
- sceanrio creaters should be elevated to building real apps
- when relying on apis, scenario dependencies and ideas stay even more free, with the feeling of building an appliaction, not jsut scripting, so the quality of scenarios will rise exponentially as more people will be willing and excited to really go for it with scenarios
- users should build apps that use DeathStarGame api or are used by it
- communicate bidirectionally over rsocket
- system and scenraios talk using data, each having its own runtime (system has several), giving natuaral isolation
- calrity and ease of development: we launch the system and then start/stop scneario app only, whereas system is always running as a whole and exposes only apis
- scnearios being apps of their own provide isolation, definitiveness of api, dependency freedom, decoupling of system and scnearios
- scenarios become true extensions, addons, or microservices to the system
- users creating scenraios build literally apps of their own, while system focuses on api communication over how-do-we-intergate-these-scripts (although we have docker and containers)
- should be possible to develop such scenario apps using the system itself
  - browser vscode would open a directory with such app's code (copied from a template directory from DeathStarGame repo)
  - system's shadow-cljs builder would add a build target and compile the app, expose nrepl, system would connect vscode to nrepl and spin up a container with this new scenario
  - now we have a repl into an app and see scenario's gui inside a tab (iframe)
  - we have a button that restarts the app container if needed (or posisbly even use VSCode's terminal into apps container, so user can restart themselves)
  - one we done, we can copy the code from the system on PC and put into a repo or smth</s>


## ~~sceanrio ui as iframe inside game ui~~

- <s>from user experience, we need to see player's scenario views and a combined view, meaning being able to switch between multiple small tabs (or open them alongide each other) inside single browser tab
- if scencario gui is an app, openable even in a standlone tab, it should be used by the as iframe</s>

<img height="512px" src="./svg/2020-11-01-iframes.svg"></img>


## installing scenarios: as namespaces

- sceanrios are simply code
- are developable only within the system, which is as it should be
- installing would mean 
  - copying files from a url into the app entity's filesystem
  - jvm app would compile(or even eval them) thus creating scenario's namespace
  - option A: ui builder would adds a new build target and compiles an app, which is used by iframe
  - option B: renderer code is directly sent by jvm-app to game-ui and evaled there

## scenario as a library: game forms the state on jvm and delivers to game-ui, which passes it to scenario renderers

- simulations run on jvm and new state is formed for that game in it's unique generated namespace
- and game passes it over its connection to game-ui and subsequently to sceanrio renderer
- as opposed to scenario updating its own renderers over unique connection
- why: game may choose to render that scenario multiple times and it should be in charge of that
- if so, than every such renderer cannot have a whole connection back to scenario app (otherwise there will be a connection for each tab rendering state of a particular player)
- or tabs for dev and latest game state for the player themselves
- point is: game should be free to render state multiple times and in a selective manner

## single container vs docker-compose: docker-compose

- since docker containers don't use much RAM and shared volume is a norm, docker-compose is a better tool and abstraction, than lower level bash scripting
- the size of the app installation (all images) does not matter, and on updates new layers will be downloaded faster

## step one: app entity contains just IPFS node and jvm-app, do data flow between peers

- this is the heart of the system


## installation/launch container

- no need for OS scripts, instead use a conatiner like a scrip ( with --rm flag , will exit after app is run)
- the container may need to be run as privileged (to be able to isntall docker app plugin), or not even, if with stack
- user runs this container, which in tern either installs docker app or uses docker stack to run the system
- it's a more powerful (we can use a ligit app inside, cljs nodejs for example, or even jvm) not just bash scripts
- user experience: just run one thing and game is up, run it again with --uninstall and it's down, run it again with --remove-volumes and that's done as well etc.


## player app eneities: identifying by port, localhost:PORT = docker-compose --project-name PORT

- so app's api/ui port also acts as an app name (or name prefix), to avoid ambiguity
- possible to encapsulate app under port ?

## installation/launch container part 2

- a privileged container that has api and can dynamically change files in sorce volume, so apps can be configured to comply to that one port
- a launch container is users/developers interface to the system: we basically put bash scripts and maybe programm api in there and can start/stop/configure system
- but: it's cross platform and versatile (a running program, not just script)
- it would also install docker app plugin or enable swarm

## how technically code evalution will work in the system

- [ palyers still need a REPL , but game state should be advancable, recreatable, syncable and independent of ui](https://github.com/sergeiudris/deathstar.lab/blob/4412eebce46dfad0f860276a2aa8d9c0e69c53c2/docs/deathstar.ltee/as-vscode-extension.md#state-its-about-state)
- [namepsaces, namespaces everywhere: namespaces can be discarded and re-created](https://github.com/sergeiudris/deathstar.lab/blob/4412eebce46dfad0f860276a2aa8d9c0e69c53c2/docs/deathstar.ltee/as-vscode-extension.md#how-to-def-namespaces-are-free)
- [discarding/creting copies of palyer's namespace in clojure](https://github.com/sergeiudris/deathstar.lab/blob/4412eebce46dfad0f860276a2aa8d9c0e69c53c2/docs/deathstar.ltee/as-vscode-extension.md#discardingcreting-copies-of-palyers-namespace-in-clojure)
- [simulation as f(state,code,time), why there is no need for cljs self-hosting](https://github.com/sergeiudris/deathstar.lab/blob/4412eebce46dfad0f860276a2aa8d9c0e69c53c2/docs/deathstar.ltee/as-vscode-extension.md#simulation-as-fstatecodetime-why-there-is-no-need-for-cljs-self-hosting)
- correction
  - code will be evaled on one mahcine - the host peer of that particular game : connected peers will send all the inputs and get eval results back
  - game simulations are run on the host peer as well (both main and test)
  - else the same: peers will get their unique namespace and copies of it, it's about creating/discarding namespaces, updating individual states (for rendering player's map) and running simultaions to advance the source of truth state of the game

## ipfs nodes, router loopback, docker compose network

- ipfs nodes (or any other apps) sometimes cannot connect due to router not supporting loopback (cannot dial your own WAN public ip)
- one approach would be to use some public ipfs node(s) that support pubsub and communicate through them, but this is fragile
- instead, make nodes able to discover/connect through docker-compose network, while keeping global ipfs node netwroking intact (by adding additional addresses to ~/.ipfs/config Addresses/Swarm
- this way, nodes can communicate locally, even offline, but would be able to connect to the game launched on another peer 

## peer conmmunication: connecting to peer vs pubsub grid - pubsub grid

- IPFS are working on making pubsub group-like: as in, peers would not particiapte in only global pubsub, but be able to join a certain group
- and they are making it more and more efficient by designing/applying new algorithms
- so looking forward, it means, that the peers of the game app would be able to form global sub-pubsub (even now it's possible with gloval pubsub)
- and would be able to exhcange data through it, as a single global app entity
- a peer would only need to connect to one other game peer and they are part of the app, can receive data
- that in principle allows us to think about data exchange (joining game, observing) as streams of data over pubsub, rather than direct connections of player peers to host peer
- however, for efficiency and speed and reducing network load, it could be better to play games peers-to-host, while doing queries and data sync over pubsub: so pubsub for everything, but a particular game via connecting
- or: a game could be a sub-sub-pubsub, so that observers can subscribe to that particular game
- with that in mind
  - when developing locally, instead of connecting to peer, go for autodiscover through mDNS (whatever that is) and forming a pubsub (global for now)

## in dev mode: be able to switch between circuit relay mode and local docker network with auto discovery

- it is awesome that communication over circuit relay just works out of the box
- but still, from design standpoint - elegance, sanity, it's peer-to-peer after all - locally launched nodes should auto discover and connect, even offline

## ipfs pubsib grid with rsocket protocol

- once peers are connected somehow, pubsub works
- what is needed is an rsocket abstraction over IPFS pubsub: be able to make p2p requests, bidirectional
  - behind the scenes, give messages id etc.

## ipfs: docker network autodiscovery, pubsub do work out-of-the-box 

- docker network autodiscovery,pubsub works out-of-the-box when on a single deafult network - nodes find each other, connect and pubsub - all just works
- docker networks probably need to be configured a bit more to allow named network auto discovery as well


## IPFS global pubsub allows for global game/player discovery staight away: it's like a radio, where app topic (and each game/event) is a frequency

- every peer will sub to the game topic: which should be like a frequency/channel
- every peer will have it's own db to store data from hosted games
- but peer's db will also store *global* lists : current players online, current games created
- when a peer creates a game, event will go into pubsub and every peer's db will update the list
- when a peer (or connected/sharing pubsub for the game peers) will query - they query one single db on the host peer's machine

## IPFS node pubsub stream cannot be consumed from jvm-app: either use libp2p or fork-modify js-ipfs node

- sadly, IPFS node's pubsub stream cannot be consumed from another app via node's API anyway...
- so basically it means either using libp2p inside an app or forking node and modifying it to expose pubsub stream via an API
- jvm-libp2p?
- the app logic should ideally be decoupled from peer logic: when we restart app, we shouldn't drop connections or lose id
- in that sense, we need a node, but such that it allows us to consume and send pubsub 
- there are also things that IPFS node stores in files (peer id, settings) that we'd have to re-implement, although the goal is access to pubsub

<img height="512px" src="./svg/2020-11-13-peer-node.svg"></img>

## how fork-modifying js-ipfs would look like

<img height="320px" src="./svg/2020-11-13-fork-modifying-js-ipfs.svg"></img>

- this agent should be written as sorf of a plugin, that compiles into a dependency and forked js-ipfs launches it in one place somewhere (and gives all the runtime refs as args)
- this way fork can move along with js-ipfs, getting updates from upstream

## ~~peer process should be part of jvm-app: evolve into better development flow (away from restarting the jvm), less moving parts, and app will deal with filesystem already~~

<img height="512px" src="./svg/2020-11-13-libp2p.svg"></img>

- <s>we want to use libp2p to build around peers and pubsub, and we don't need IPFS node
- both libp2p and IPFS node have a lot of features that are not part of the game, so should not focus on those, let others handle them ok
- when app is working, it will be naturall to have a peer logic running a process of jvm-app
- the only reason - bad reason - to do it over rsocket as a standalone container is misconception about easier development flow: it's clear how to restart docker container, that's why it is tempting to build processes as docker containers (although in this case it's unnecessary)
- instead improve jvm development flow: design such that processes can be restarted, or simply avoid running frequently (rather build-build-build-run)
- but eventually - the sooner the better - there need to be a way to restart logical async processes without restarting jvm
- jvm will will have
  - libp2p peer process
  - an rsocket connection to ui
  - (possibly) rsocket to vscode editor
  - db connection
  - filesystem access
- and it is great to build those as processes, but be able to handle main async flow around these abstractions inside a single runtime with core.async
- so by design of the peer, libp2p peer process should be part of the jvm-app</s>

## ~~one app, one jvm: game is the focus, not development~~

<img height="512px" src="./svg/2020-11-14-one-app-one-jvm.svg"></img>

- <s>app is a single jvm process, a peer node
- editor should be part of the game, start simpliest and evolve overtime
- db should be embedded into jvm 
- we have to deal with filesystem anyway, so no problem in doing so from jvm
- so many moving parts go away: editor-to-jvm communication, unnecessary gui containers, traefik
- app should be distributed as installable desktop app 
- jvm and REPL is the perfect environment for development, should be the focus
- app can serve gui two ways
  - via embedded http server into browser (from resources directory)
  - via javafx
- if javafx, have to deal with javafx, but have simplier setup: only need one repl (probably)
- if browser, no need for javafx, but have to do rsocket and http server and a cljs repl
- gui app dev tool can even come with the app if needed (should not be, game is the focus)
- editing scenarios first will be through external editor, but oovertime the ingame editor will evolve and be sane enough
- the issue with config files in develpment: how to start multiple instances
  - either launch the app with config file argument
  - or select config after load from directory
  - or use db for that and kind of "login" into that player's profile via db 
- random ports and mDNS: use libp2p's random ports feature so that app instances can find each other naturally</s>

## ~~use browser gui instead of javafx: one app with gui in browser on localhost:port~~

<img height="512px" src="./svg/2020-11-15-one-app-one-jvm.svg"></img>

- <s>javafx brings unnecessary complexity to ui, which is needed to be web page anyway
- http server and GUI on port brings simplicity to development: can have a docker-compose with peer0 peer1 peer2 ... services, each having volume to .peer1 .peer2 .peer3
- a seprate GUI dev container will be used for building ui, but files will be output directly into jvm /resources folder
- http-server will run right on jvm and simply serve static files
- each peer container will expose one port for gui (and for REPL during develpment)
- jvm-app running inside container **can be easily run as uberjar on any system**
- it's a single app that has a renderer in browser on localhost:port</s>

## ~~libp2p as embedded nodejs process into jvm~~

<img height="512px" src="./svg/2020-11-16-one-app-one-jvm.svg"></img>

- <s>js libp2p is actively developed and used
- instead of running libp2p-jvm, which is yet unclear, better to run a nodejs process inside jvm that does libp2p peer logic
- there should be simple interprocess communication, so that we can start/stop the node process, or ask/send data from jvm/repl</s>


- wrong, wrong: cannot have an uberjar and a nodejs app running in it, unless it's graalvm
- even if it is, it's a trick
- why? because **node_modules** and **native_dependencies** - those cannot be part of `some-script.js`, so **`npm i` is unavoidable by design** 
- so we cannot rely on things like `libp2p-tcp` nad `libp2p-mdns` to be includable into a `script.js` - they may have native deps (and likely do, cause they trigger node-gyp)
- *so if we want to use js-libp2p, it can only be run with `node app.js`, not from jvm`

## on how to restart IPFS node container from within docker deployment: use docker-compose's restart:on-failure property 

- send shutdown singnal to deamon (from main container via http api) /v0/shutdown
- the container will stop, but docker will restart it because of restart:always or restart:on failure feature
- if containers can be stopped and restarted like this (for example to apply IPFS config changes), no need for single docker container, can go docker-compose deployment

## docker-compose and fork-moifying ipfs node

- docker-compose as outlined above
- ipfs node, because it has features, js - because we can use cljs 


## acquiring rsocket channel (or stream) on each ipfs pubsub sub topic

- app will use peernode's api to sub to a topic via request-stream
- each topic subscription can have it's own rsoket stream on a seprate channel (if it makes sense, maybe not, maybe all vals on same ops| channel)
- but if we use, it's powerful: each game can be cleanly represented by one rsocket stream
- again, it's an option: maybe we'll use :game/uuid anyway


## system operations should be explict in specifying request-response request-stream fire-and-forget request-channel

- explitely, as part of github.cljctools/csp
- the idea of values(ops) in the system are decoupled from operation how is false: it makes system and ops ambiguos, unclear and decision-less
- ops should explicetly say: "this is a stream request and here are responses for it", "this is a request-response request and here's response"
- so an op specifies
  - op-key : op's name
  - op-type: request-response request-stream request-channel fire-and-forget
  - val-type: request or response
- when we request an rsocket channel, we proabably cannot do (and shouldn't,by design) request-responses within it (it's likely not rsocket inside rsocket)
- so in code, after request-channel, we'll get :send and :recv channels, and all values - in and out:
  - are fire-n-forget: neither response nor requests
  - or initiator's values are :requests and acceptor's are :responses (if in app these are two different values, but same :op-key and a distinction is needed)
- fire-and-forget: we should explicitely specify or if value has no key it's fire-and-forget
- request-response: we specify :request-reponse and - :request or :response
- request-stream : the intial value has :val-type :request, the streamed values can be either :response or not specified

## cljctools/csp probalby should not have rsocket op types

- we have channels, core.async channels
- request-stream means "take this (chan) and establish a stream over rsocket, so that this (chan) receives values"
- it does not and should not specify which values
- so basically, it always bi-drectional request/response or fire-nad-forget, but with an incredible ability to acquire streams and channels
- for example, app will request-stream from peernode, get a channel and all pubsub messages will be conveyed onto this channel, but in code - it's just a channel, and values are unaware (well, it's not a problem to have unused value keys, it's just that cljctools/csp probalby should not have rsocket op types)
- request stream means one channel
- request cahnnel means two channels (:send and :recv)
- fire-and-forget means use the default channel
- reqeust-response measn use the default channel
- nah, even simpler
  - request-response is a special request-stream of one value
  - request-stream is get a stream of many values
  - fire-and-forget is a psecial stream of one value 
  - request-channel is two stream of values

## would not it be better to explicitely say 'these are responses to request-stream operation'?

- say, `cljscootls/csp` specifies `request` `response` `fire-n-forget` `request-stream` `request-channel` and also no specifier 
- if some process asks another `(some-proc.chan/some-data-please-as-stream`) - that particular opeation specifies `request-stream`, but all the stream values are the same operation (name) but with no op-type
- so rsocket op-types are used for requests, and reponses (unless it request response) are values with no op-type
- and {:op-key some-op :op-type :request-stream} is one operation, but {:op-key some-op} is a different
- so the question is: would not it be better to explicitely say "these are responses to request-stream operation"?

## why full op-type op-orient op-key spec can matter: piping into ine channel and distinguishing

- a process can pass for example an ops| channel to request-stream, and all stream values are then put on ops|
- so ops| needs to explicitely condp on "hey, this is the case of :some-op :request-stream :response" 
- but isn't it a bad design? shouldn't we isntead use (alts!) and multiple channels?

## simple: there is no contradiciton, qualifying an op is a choice

- cljstools/csp has the spec for all ops
- an we explicitely request-stream etc.
- but the values on that stream channel are apps decision: can be fully qualified can be unqualified: because it's an op defined for app's proc, and we can define it either as fully qualified op or as unqualified op and directly put those onto a channel
- in regards to choice: fully qualified seem better and more explicit


## first and most important feature? being able to add own scenarios from day 1

- make it so that peers can add their own scenario files, even by changing existing ones
- so the game is open and unlimited in scenarios from day one
- else next
- so first
  - a list of scenarios, can install/uninstall by repo:hash/path  
  - can host a game and play with other

## game is like a an upstream repo and forks

- a host runs sceanrio generation
- creates files locally and sends to data to peers, they also create generated game files
- we open with code-server code in the browser and eval in our own dev namespace
- other peer's evals are intercepted (from single local nrepl) and sent to others, re-evaled for each player
- so it's a sychronization of namespaces by broadcasting eval ops
- losing eval ops does not matter: a host peer runs the game's main process and timer, and on time asks other to submit the whole namespace
- it get's evaled once (like a program) and it's main or other scenario defined functions are rused in a simulation that runs on the host - and the state is broadcasted as we go
- or better: the current state and code is sent to everyone and everyone runs the simulation, so it's fast
- so things run locally actually, and we sync state over pubsub, and **one peer** is - by trust - the source of truth
- a game is like a an upstream repo and forks: every peer has the full version and state of the game, if upstream goes away, another player becomes the host
- the winner is decided by the state of the host (upstream)
- creating a game is creating a pubsub topic
- every peer's has namespaces of other peers, which are synced over pubsub
- simulations are run on each peer, and winner is decided by each peer, after the game is complete peers echange the list of [peer game-results] and one peer is marked as host, so by trust their game-results are true 

## game flow from creation to completion

- peer creates a game: generates a unique uuid, subs and publishes to it as topic (frequency)
- the uuid is either shared with other peers by pegeon or owl, or there is a galobally synced list of games
- first, for invite only game, the game topic (frequency) should be shared with invitees
- peers emit regular ping on the frequency to signal that they are in the game, every peers sees the list
- ok, host peer - which at this point is simply bu trust and convention - runs the scenario generation, creates files and sends to others, they create game files on disk as well
- peers create namespaces game-id.peer-id
- host peer starts the scenario process (timer), publishes game start, other peers receive and start the timeer as well
- peers can eval in their namespace, that changes state (atom),each atom is synced to local browser ui (can see tabs for each peer)
- first scenario - rover on mars: there is a map (tiles) and we want to program rover (write a function or two) to score better on that map
- peers eval locally to try to score, reset and try again, all via REPL
- by the end of the timer, we need to save the file with functions that we think will score best
- all that - happens on every peer and we can see each peer's map and rover moving when they eval and their score
- so rover scneario - is single player, who scores most
- timer is up, repl is paused, each peer's namespaces is sent to others, evaled and each player runs all player's simulations and get's a list of scores
- peers exchange the list of scores, the winner is the highest score on the host peer's list

## encourage scenario creation in conrtibutor's repos, like deathstar.lab/scenarios

- scenarios coming with the game is a list of such deps (repos) with scenarios
- when game launches, it creates a docker volume and installes scenarios there (deps cli puts cache into /root/.clojure )
- also the gui is compiled and put into volumes
- or put it into ctx volume
- the jvm-app has the volume available and can read fs and show in ui which scenarios are installed
- compiler comes with the game: on every game launch it goes over the default list of scenarios and added by user and deps-installs and shadow-compiles missing/new ones
- so evan default scenarios should be a list of repo:hash/path, like deps, and are installed on first launch
- the same mechanism for new scenarios

## installing a scenario as a user: open a installed scnearios deps.edn and add entry, press install

- user can have an extended list of scenarios, installed scenarios list cannot have multiple version for one sceanrio
- so it should be 
```clojure
; https://clojure.org/reference/deps_and_cli#_dependencies
{github.username.deathstar.lab.scenarios/rovers {:git/url "https://github.com/username/deathstar.lab"
                                                 :sha "988b8191c6b31ece0cfcef6d15111efda999f262"
                                                 :deps/root "./scenarios/rovers" }}
```

## users manually adding scenarios by modifying deps.edn and source files to import namespaces?

- ui's deps.edn points to deps.edn with scenarios' UIs
- app's deps.edn points to deps.edn with sceanrio itself
- user can open and change both files using code-server - add more scenarios
- the ui watches the files, so it automatically installes the scenario and recompiles ui
- but: if namespace is not required, it won't get added into bundle...
- but: if peers open an actual .cljs file that imports scanerios namespaces, than it's in
- so scenario installation is actually changing the source code of the game? 
- same way a scenario can be added to app itself (to namespace listing scenarios)


## we create the game first (pubsub frequency, room), then host picks the scenario (or vote)

- peers join the game
- then host selects the scenario
- it the same thing as scenario first, but in terms of game page design: we first click generic "create game" button, means frequency, than page opens, and we configure the game

## heart beats, waves: peers send state as a heartbeat, can use timeout to make decision - elect a new host for example

- host creates the game and emits state periodically and on a request (when a peer joins)
- peers get state/upadates and reset/merge
- if say, 5-10 seconds pass without any event from host, game shows: "elect new host" - or simply, use timestamp, the first joined player is the new host
